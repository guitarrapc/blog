---
Title: Amazon EKS Auto Modeを使っていく
Category:
- AWS
- Kubernetes
EditURL: https://blog.hatena.ne.jp/guitarrapc_tech/guitarrapc-tech.hatenablog.com/atom/entry/6802418398335749389
PreviewURL: https://tech.guitarrapc.com/draft/entry/ln7wiHg0Cs_VGd1Q8TUWIQBJH6k
CustomPath: 2025/03/09/235900
---

2024/12/1にre:InventでEKS Auto Modeが[発表](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/amazon-eks-auto-mode/)されました。AWSとしては、EKS Auto Modeを今後EKSを構築する時のスタンダードにしていきたいんだろうなと感じます。ただ、Auto ModeはEKSアドオンやKarpenterをはじめ、組み込みコンポーネントが隠蔽されており違いを知っておかないと戸惑う感じがあります。

今回はEKS Auto Modeを構築しつつ、どのような挙動なのか見ていきます。

[:contents]

# EKS Auto Modeのコンセプト

EKS Auto Modeの[コンセプト](https://aws.amazon.com/jp/blogs/containers/getting-started-with-amazon-eks-auto-mode/)を見ると、クラスター管理をAWSにオフロードしてアプリケーション開発へフォーカスできることを狙っているようです。その例として、次の図が示されています。

これまでのEKSではAddOnsやEC2インスタンスをユーザー管理していたのに対して、Auto Modeはストレージ/コンピュート/ロードバランサー管理をAWSが担保しEC2にも関与するようになっています。

> Before Auto Mode
>
> ![image](https://github.com/user-attachments/assets/7199547b-cfd5-4db4-a4da-7fa790fcbe63)

> After Auto Mode
>
> ![image](https://github.com/user-attachments/assets/431bfd3e-b446-44a8-9c33-97ea5d83a61b)

# コンセプトから受ける印象と実情

コンセプト図から、EKS Auto Modeは次のような印象を抱きます。

* EKSアドオンはAWSが管理するためユーザーは導入不要になった
* EC2のオートスケールはAWSに完全お任せできる
* IngressからLB作成や管理、TargetGroup紐づけも任せられる
* 作成されたALBやEBS、EC2は引き続きユーザーが関与できる

実際使ってみると、期待に近いものの従来のEKS管理から見るとそこまで楽になったかは悩ましい側面もあります。また、以下2点はコンセプトから読み解けないでしょう。

* Helmはどうなるのか
* Podの水平スケールがどうなるのか

それぞれ見ていきましょう。

## EKSアドオンはAWSが管理するためユーザーは導入不要になった

これは受けた印象そのままです。EKSアドオンを追加不要、Podも見えなくなったのでマネージドだなという印象と実感が揃っています。ユーザーが追加したEKSアドオンは従来通りPodが見えるため違和感はありません。

* ✔️: これまでEKSクラスターで必須といってもよかったCoreDNS、kube-proxy、VPC CNIはAuto Modeでは導入不要
* ✔️: Node LocalDNSも組み込みになった
* ✔️: AWS EBS CSI Driverも組み込みになった
* ✔️: EKSPod Identityt Agentも組み込みになった
* ✔️: 組み込まれたコンポーネントのPod等はkubectlで見えずNodeも利用しない
* ⚠️: サードパーティEKSアドオンの新Kubernetesバージョン対応は引き続き待つ必要がある
* ⚠️: Mertics Serverは組み込まれておらず、組み込みじゃないEKSアドオンを追加するとPodが起動、Nodeも起動する

組み込みになったEKSアドオンやコンポーネントはEKSで必須でしたがこれまでは手当が全然なくてつらかったので、Auto Modeで組み込まれたことは非常に好ましいです。
組み込みリソースはPodが見えないだけじゃなく、これらのリソース用はNodeが存在しないのに動作するのが不思議な感覚です。

とはいえ、ユーザーが追加したEKSアドオンはPodなどリソースが見えますし、Nodeも必要になります。贅沢をいうなら、Metrics ServerはHPAでもKEDAでも必須なので組み込んでほしいものです。

EKSアドオンの注意点として、3rdパーティのEKSアドオンが利用できるかはEKSのKubernetesに依存します。例えば、現在EKS 1.32が最新ですが3rdパーティEKSアドオンの多くは対応していません。各Addon提供者の意欲に依存しているので、EKSアップグレードの可否がEKSアドオンに左右されうるのは嫌だなと感じます。

## EC2のオートスケールはAWSに完全お任せできる

完全お任せにはできませんが、限定的なシーンではお任せできます。

* ✔️: EKS Auto ModeはKarpenterコントローラーを組み込みで持っている
* ✔️: 任せる場合に使われる`general-purpose`ノードプールは、SPOTは使わない、amd64でしかアプリケーションを動作しない、ストレージサイズは80GB固定、インスタンスタイプもある程度限定される
* ⚠️: 上を1つでも変えたい場合、自分でKarpenterのNodeClass[^1]、NodePool定義を書く必要がある
* ⚠️: AMIは常に最新が利用される(自動パッチ)
* ⚠️: EKS Auto Mode管理のKarpenterで作られたEC2は追加料金がかかる(+11-12％程度)

EKS Auto Modeのノード水平スケールはKarpenterなので、Karpenter 1.2以上を使っている人なら戸惑うことなく自分のNodeClassやNodePool定義を書くことができます。Auto ModeのKarpenterはOSS版Karpenterと全く一緒ではなく、NodeClassとNodePoolのフィールドキーにカスタマイズが入っています。特に注意が必要なのはNodeClassです。EC2NodeClassの代わりに用いるのですが、`amiSelector`がなくなったことでAMIを指定せずとも最新AMIが使われます。AMIを固定できなくなっているので、Linux系のセキュリティインシデント時の対応時に注意が必要そうです。

気になる点はコストです。EKS Auto Modeの組み込みKarpenterで起動したEC2インスタンスは追加料金が11-12％程度かかります。オンデマンド・スポットにかかわらず固定金額かかるので、単純にEKS Auto Modeにするとコストが12％程度上がると考えても差支えないでしょう[^2]。また、組み込みノードプールはSpotが利用されないので、どのみち自分で書くことになります。

**EC2コスト増加分の個人的所感**

Karpenterコントローラー・従来EKSアドオンのPodが見えないことから、EC2コスト追加料金はこれらの稼働分としてかかっているのかなと感じます。しかし、自前で動かしてもKarpenterコントローラーで2Pod動作、EKSアドオン各種で6-10pod程度増える程度です。これに対して、全EC2ノードに11-12％コスト上乗せするのはちょっと納得感ありません。

SPやRI、スポットでも1台当たり追加料金は変わらないので、費用低減策をしているほど重くなりますよね。

## IngressからLB作成や管理、TargetGroup紐づけも任せられる

完全お任せにはできませんが、限定的なシーンでお任せできます。TargetGroupBindingは使えないので、ALBはIaCで作って万が一消えないようにしているという人は注意しましょう。

* ✔️: EKS Auto ModeはElastic Load Balancingを管理するコントローラーを組み込みで持っている
* ✔️: ALBやNLBをIngress/Serviceで管理する場合は任せられる
* ⚠️: 既存のALBをEKS Auto Modeに直接マイグレートできない
* 🆖: [TargetGroupBindingに対応していない](https://github.com/aws/containers-roadmap/issues/2508)

EKS Auto ModeではAWS LoadBalancer Controllerを組み込みで持っていると言及しておらず、annotationsではなくIngressClassParamsで構成調整に代わり、フィールド名も異なっているので独自の仕組みになってそうです。

もしもAWS LoadBalancer Controllerを使っている既存EKSクラスターをAuto Modeに変更する場合、[ドキュメント](https://docs.aws.amazon.com/eks/latest/userguide/migrate-auto.html)に沿って、DNSベースのトラフィックシフト(Route53の加重ルーティング)を使うのがいいでしょう。[^3]

## 作成されたALBやEBS、EC2は引き続きユーザーが関与できる

組み込みKarpenterで作られたEC2(Auto Mode管理ノード)は、ユーザーの関与が可能な側面とできなくなった側面があります。EC2を直接消せなくなったのは安全ですが、一方で夜間停止などのアプローチ方法が1つ減りました。[^4]

* ✔️: EKS Auto Modeで作成されたEC2、ALB、NLB、EBS等はAWSコンソールなど従来通り確認できる
* ⚠️: Auto Mode管理のノード(EC2)はユーザーが消せない、ソフトウェア追加できない、SSHできない
* ⚠️: Auto Mode管理のノードは最大21日寿命でPDBを使った制御が必須

マネージドインスタンスの[ドキュメント](https://docs.aws.amazon.com/eks/latest/userguide/automode-learn-instances.html)にある通り、EKS Auto Modeで起動したEC2はEKS管理下になるため、パッチあて、ソフトウェアインストール、sshはできなくなっています。セッションManagerがアクセスできないの悲しいですね。

![image](https://github.com/user-attachments/assets/f658a1ec-a267-44eb-8297-35dad3fcbab6)


特に最大21日寿命なのはワークロードを選びます。ゲームサーバーでよくある「インメモリにデータを持つサーバー」は、時にワールドサーバーと呼ばれる大量常時接続をさばき、長時間起動しっぱなしにするケースがあります。このケースではワールドサーバーの停止はゲームの停止を意味するため、サーバーの入れ替えもタイミングを選ぶことになります。そう、寿命日数とハードリミットされるのと致命的に相性が悪いんですね。
また、寿命があるということは起動タイミングをユーザーが気にしない以上は、PodがいつNodeから追い出されるかタイミングが読めないことを意味します。このため、PDBを使ってPodがどのように入れ替わるか制御する必要があります。PDBを指定しない場合、Podがいつの間にか0のタイミングができます。

## Helmはどうなるのか

Auto Modeになっても、Helmは引き続きユーザー管理です。それはそう。そして、EKSで一番つらいのはHelmでもあるので難しさは解消しません。Helmアップグレードは地獄安定。

* ⚠️: EKS Auto Modeでもユーザーが自分で導入したHelmは自分で管理が必要

利用するHelmをすべて3rdパーティEKSアドオンにすれば楽になる、と考えてしまいそうですが、EKS AddoごとにKubernetesバージョン対応を待つ必要があります。Kubernetes 1.32への対応が3/15時点でもあまり進んでない現状からすると、引き続き自前Helmで導入を選ばざるを得ないケースが多いでしょう。がんばりましょう。

とはいえ、Karpenter、AWS LoadBalancer Controller、EBS CSI Driver、Pod Identity Agent、Node LocalDNSの導入が省けるので多少は楽になりますね。コスト増加と見合ってるとは言い難いですが。

## Podの水平スケールがどうなるのか

EKS Auto ModeはNodeの水平スケールは管理しますが、Podの水平スケールは関与しません。

* ⚠️: EKS Auto ModeはHPAが利用できる。KEDAを使うなら自前Helm導入が必要

HPAはその単純さから、Kubernetes標準で提供されるPod水平スケールとしては悪くないと考えています。しかし、実際のワークロードはCPU/Memoryではなくキュー残数やリクエスト処理時間などアプリケーションメトリクスをベースにスケール判定することが多いでしょう。また、時間制御、0台制御、一時的なmin増加でのスケールアウトなどを考えると、HPAでは不十分です。

この意味でEKS Auto Modeはあくまでもノードレベルの水平スケールまでは管理するが、Podはアプリケーションがやるべきという切り分けが見えます。それはそう、とはいえKEDAぐらいは入れてほしかった。HPAは厳しい。

# EKS Auto Modeは使えるのか

あまり手をかけることがない小規模な環境(開発や検証環境含む)でEKS Auto Modeは使いやすいと感じます。一方で、大規模な環境(本番環境など)でEKS Auto Modeはコスト面から説得力は持ちにくそうです。特に、開発でEKS Auto Mode、本番でEKS Auto Modeとしてもコントローラーや定義の管理からすると共通化できず嬉しくないのもペインポイントです。

* ✔️: 小規模な環境 (追加Helmがない、EC2台数が少ない)
* ⚠️: 中～大規模な環境 (追加Helmが多い、EC2台数が多い)

EKS Auto Modeのコストさえ説得力を持たせられればいいのですが、現時点の料金体系では大規模環境でEKS Auto Modeを採用する動機付けは難しいと感じます。また、小規模な環境なら、ECS FargateやLambdaで組んでしまってよいケースは多く、EKSを使う環境で小規模とはというのも難しいです。

AWSコンテナ系アプリケーションで見たときに、クラスターバージョン含めた管理の楽さで`ECS Fargate > ECS EC2 >> EKS Auto Mode > EKS`、コスト面で`ECS EC2 > EKS > EKS AutoScale > ECS Fargate`[^5]という感触です。

## EKS Auto Modeが実現していること

EKSはマネージドKubernetesと言いつつコントロールプレーン・AWS-Kubernetesアクセス管理・ログ回りなのでは…というこれまでを考えると、EKS Auto ModeはよりAWS統合が強くなっています。

必須EKSアドオンは考慮不要になりました。Node水平スケールアウトも組み込みKarpenterに任せることができ、EC2の自動更新もしてくれます。ALB/NLBとの統合も任せることができます。EBSマウントもサクッとできますし、Pod Identityもさくっと利用できます。お任せ度が高まっているのは事実です。

## EKS Auto Modeのペインポイント

コストとこれまでの運用との差異が見受けられるので、そこに合致するとつらさがあります。EKS Auto Mode管理のEC2は一台あたりコストが+11-12％上がります。ALBの安定保持のためIaCでALBを作ってアプリケーションとTargetGroupBindingでつなぐ安全策が取れません。EC2は最大21日寿命で入れ替えタイミングは制御困難です。

また、ユーザー自身がExternalDNSやExternalSecretsのような各種コントローラーをHelmで導入している場合、Helm更新時の手間は軽減していません。PodスケールアウトはPDBしか持っていないため、時間スケールや0 replicasなどが必要ならKEDAを入れざるを得ません。これらは日常的な運用であるため、EKS Auto Modeといっても運用が完全に任せられるわけじゃないです。

## EKS Auto Modeの採用基準

KubernetesとAWS統合部分が楽になった一方でコストが上がるため、大量のEC2が必要なワークロードではコスト増加が受け入れられるかはカギになるでしょう。また、従来KarpenterやAWS LoadBalancer Controller(ALBC)をはじめとして複数Helmコントローラーを利用していたチームにとっては、Karpenter/ALBCの管理が不要になっても他のHelm管理は残るため楽になったとは感じられない[^6]のが正直なところです。

導入を検討するフローはこういうかんじでしょうか。

<details><summary>フローチャート</summary>

```mermaid
graph TD
  a([EC2の稼働台数])
  b([Helmコントローラー])
  c([ALBロードバランサー管理])
  A[EKS Auto Modeの利用を検討する]
  B[EKS Auto Modeは利用しない]

  a --少ない---> b --少ない---> c --ALBCで管理---> A
  a -.多い..-> B
  b -.多い..-> B
  c -.IaCで管理..-> B
```

</details>

![image](https://github.com/user-attachments/assets/d64f4ae5-fb15-4e8f-97bf-42bbb16331aa)


# EKS Auto Modeを使いつつ違いを見る

実際に触ってEKS Auto Modeの挙動を確認しましょう。チェックするAuto Modeの次の違いです。

1. 組み込みコンポーネントがある
2. 組み込みノードグループがある
3. 組み込みKarpenterで動作させるEC2は追加料金がかかる
4. 求められるIAMポリシーが異なる
5. ネットワーク周りで制約がある
6. 組み込みでALBを作成できる

なお、Auto Modeでも接続やロギングは共通です。

* EKSの認証方法は変わらない
* EKSロギングは変わらない

EKS Auto Modeを追加EKSアドオンがない状態で構築して、そこから違いについて順番に見ていきましょう。

## 組み込みコンポーネントがある

EKS Auto Modeで何もEKSアドオンを追加していない状態でも、EKS Auto Modeには組み込みコンポーネントがあります。

![image](https://github.com/user-attachments/assets/9136c64e-b1ba-462f-944e-be4f94670396)

組み込まれているのは従来EKSアドオンで入れていたものや、Karpenterを含めたAWS連携部分に関わるコントローラーです。EKS Auto Modeの組み込みコンポーネントは[次の通り](https://docs.aws.amazon.com/eks/latest/userguide/auto-upgrade.html)です。

* 必須EKSアドオンだったkube-proxy/CoreDNS/Amazon VPC CNIが組み込みインストールされ、Podは隠蔽されている
* Karpenterが組み込みインストールされており、Karpenterコントローラーは隠蔽されている
* AWS LoadBalancer Controllerが組み込みインストールされており、ALB Ingress Controllerは隠蔽されている
* AWS EBS CSI Driverが組み込みインストールされている
* EKS Pod Identity Agentが各ノードでインストールされている

一方で、自分でインストールしたHelmやアプリケーションはこれまで通り自分で管理します。自分でEKSアドオンから追加したコントローラーも自分で管理します。

組み込みコンポーネントの面白いところは、EKS Auto Modeで起動してEKSアドオンやアプリケーションを何もデプロイしていない状態でPodを見ると何も起動していないことです。特にPodを起動していないと、NodeClassやNodePoolも見えないのが興味深いです。[^7]

```sh
$ kubectl get po -A
No resources found
$ kubectl get nodeclass
No resources found
$ kubectl get nodepool
```

## 組み込みノードグループがある

EKS Auto Modeは`system`と`general-purpose`という組み込みNodeGroupがあります。従来のNodeGroupとは異なり名前が決められており、使うかどうかは任意です。

![image](https://github.com/user-attachments/assets/16e402f8-a021-4369-b849-71344ddea339)

組み込みノードグループの定義を見てみましょう。組み込みノードグループはKarpenterで動作しており、組み込みノードグループのKarpenter定義は変更できません。

組み込みノードグループは、NodeClass `default`を共通利用してしています。NodeClassは、Karpenterでいうところの[EC2NodeClass](https://karpenter.sh/v1.3/concepts/nodeclasses/)に相当し、NodePoolで起動するEC2のスペックを定義します。EC2のスペックはEKS Auto Modeで建てたクラスターと同じセキュリティグループ、サブネットグループになっているのが特徴です。Karpenterではセキュリティグループ、サブネットグループともにタグで検索させるのが主流だったので、これは組み込みの性質を強く感じます。また、KarpenterのEC2NodeClassでは`amiSelector`でAMIバージョンを都度指定する必要がありましたが、NodeClassで指定はできず最新AMIが使われます。

組み込みノードグループの`NodePool`はKaprneterの[NodePool](https://karpenter.sh/v1.3/concepts/nodepools/)と同じスペックですが、`requirements`の一部フィールドがEKS [Auto Mode独自](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/create-node-pool.html)になっています。

![image](https://github.com/user-attachments/assets/a5a28ef1-2f12-450e-8db5-75c7fd90f7c5)

それぞれの定義を見てみましょう。

<details><summary>NodeClass `default`</summary>

```sh
$ kubetl get nodeclass default -o yaml | kubetl neat
apiVersion: eks.amazonaws.com/v1
kind: NodeClass
metadata:
  annotations:
    eks.amazonaws.com/nodeclass-hash: "3399735243323253970"
    eks.amazonaws.com/nodeclass-hash-version: v1
  labels:
    app.kubernetes.io/managed-by: eks
  name: default
spec:
  ephemeralStorage:
    iops: 3000
    size: 80Gi
    throughput: 125
  networkPolicy: DefaultAllow
  networkPolicyEventLogs: Disabled
  role: automode-eks-AmazonEKSAutoNodeRole
  securityGroupSelectorTerms:
  - id: sg-1234567890
  snatPolicy: Random
  subnetSelectorTerms:
  - id: subnet-1234567890123
  - id: subnet-1234567890234
```

</details>

<details><summary>NodePool `system`</summary>

`general-purpose`と違い、traintsに`CriticalAddonsOnly`があり、arm64アーキテクチャにも対応しています。CoreDNSなど多くのEKSアドオンがこのtaintsをもっているので、EKSアドオン専用のNodePoolを指向していることがわかります。

```sh
$ kubectl get nodepool system -o yaml | kubectl neat
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  annotations:
    karpenter.sh/nodepool-hash: "4982684901400657622"
    karpenter.sh/nodepool-hash-version: v3
  labels:
    app.kubernetes.io/managed-by: eks
  name: system
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
        - arm64
      - key: kubernetes.io/os
        operator: In
        values:
        - linux
      taints:
      - effect: NoSchedule
        key: CriticalAddonsOnly
      terminationGracePeriod: 24h0m0s
```

</details>

<details><summary>NodePool `general-purpose`</summary>

systemと異なりtaintsはなく、amd64アーキテクチャのみ対応しています。オンデマンドでしか起動しません。

特に制約がないため、Auto Modeで適当にPodを起動すると、`general-purpose`NodePoolでノードが作成されます。

```sh
$ kubectl get nodepool general-purpose -o yaml | kubectl neat
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  annotations:
    karpenter.sh/nodepool-hash: "4012513481623584108"
    karpenter.sh/nodepool-hash-version: v3
  labels:
    app.kubernetes.io/managed-by: eks
  name: general-purpose
spec:
  disruption:
    budgets:
    - nodes: 10%
    consolidateAfter: 30s
    consolidationPolicy: WhenEmptyOrUnderutilized
  template:
    spec:
      expireAfter: 336h
      nodeClassRef:
        group: eks.amazonaws.com
        kind: NodeClass
        name: default
      requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values:
        - on-demand
      - key: eks.amazonaws.com/instance-category
        operator: In
        values:
        - c
        - m
        - r
      - key: eks.amazonaws.com/instance-generation
        operator: Gt
        values:
        - "4"
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
      - key: kubernetes.io/os
        operator: In
        values:
        - linux
      terminationGracePeriod: 24h0m0s
```

</details>

## 組み込みKarpenterで動作させるEC2は追加料金がかかる

EKS Auto Modeが管理するNode(=EC2)に対して追加料金がかかります。

> Amazon EKS Auto Mode の料金は、EKS Auto Mode によって起動および管理される Amazon EC2 インスタンスの期間とタイプに基づいてお支払いいただきます。以下の Amazon EKS Auto Mode の料金は、EC2 インスタンス自体を対象とする Amazon EC2 インスタンス料金に加えて請求されます。EC2 インスタンス料金と同様に、EKS Auto Mode の料金は 1 秒単位で課金され、1 分間分の最低料金がかかります。オンデマンド、1 年および 3 年のリザーブドインスタンス、Compute Savings Plans、スポットインスタンスなど、EKS Auto Mode では Amazon EC2 インスタンス購入オプションをすべて利用できますが、EKS Auto Mode の料金は EC2 インスタンス購入オプションとは無関係です。
>
> 引用: https://aws.amazon.com/jp/eks/pricing/

まとめると次のようになります。

| 起動タイプ | 追加コストが必要 |
| --- | --- |
| EKS Auto ModeのBuilt-in node poolsで起動したEC2 | 必要 |
| EKS Auto Modeで自前Node ClassとNodePoolで起動したEC2 | 必要 |
| EKS Auto ModeにOSS Karpenterを入れてEC2NodeClassとNodePoolで起動したEC2 | 不要 |

EKS Auto Mode管理のNodeClassを避けて独自NodeClassを作ってもAuto Mode管理のノードとカウントされるので抜け穴はなさそうです。なお、追加分のコストはCost Explorer > API operationでEKSAutoUsageとして表示されます。(Service分類は`Elastic Container Service for Kubernetes`)

**Auto Mode管理のノードか判別する**

EKS Auto Modeが管理しているNodeどうかを識別するには、EC2インスタンスなら`Managed`にTrueがついているか、Nodeなら`eks.amazonaws.com/compute-type: auto`ラベルがあるかで判別できます。

![image](https://github.com/user-attachments/assets/2193595c-021a-4d4b-8dac-fb90c9e44e1e)

Nodeにラベルがあるので、nodeSelectorやtains/tolerationsで制御はできますね。

```yaml
nodeSelectcor:
  eks.amazonaws.com/compute-type: auto
```

**追加料金の試算**

例えばEKS Auto ModeのBuilt-in node pools`system`が有効な状態で、EKSアドオンの`Metrics Server`をインストールすると`c6g.large`が2台[^8]起動します。東京リージョンで起動した場合、EKS Auto Modeで起動した`c6g.large`のコストは`$0.0856 (EC2オンデマンド料金) + $0.01027 (EKS Auto Mode追加分) = $0.09587/h`と元の価格から見て119.9％です。

AWSの例示する料金例を見ても、おおむね+11-12％程度の追加EC2料金になると見込むことになりそうです。

![image](https://github.com/user-attachments/assets/12718e9d-d206-497a-81ce-150c9cd0f113)

![image](https://github.com/user-attachments/assets/6de4f6f6-9bd0-4942-aec1-505a776b3065)

## 求められるIAMポリシーが異なる

通常のEKSクラスターとAuto Mode EKSクラスターで「EKSクラスター用IAMロール」「ノード用IAMロール」の必要ポリシーが異なります。詳細は[ドキュメント](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/auto-learn-iam.html#tag-prop)を見るといいのですが、組み込みコンポーネントで必要なIAMポリシーが追加された感じです。

| IAM Role | 通常のEKSクラスター | Auto Mode EKSクラスター |
| --- | --- | --- |
| EKSクラスター用IAMロール | `arn:aws:iam::aws:policy/AmazonEKSClusterPolicy`<br/>`arn:aws:iam::aws:policy/AmazonEKSVPCResourceController` | `arn:aws:iam::aws:policy/AmazonEKSClusterPolicy`<br/>`arn:aws:iam::aws:policy/AmazonEKSComputePolicy`<br/>`arn:aws:iam::aws:policy/AmazonEKSBlockStoragePolicy`<br/>`arn:aws:iam::aws:policy/AmazonEKSLoadBalancingPolicy`<br/>`arn:aws:iam::aws:policy/AmazonEKSNetworkingPolicy`<br/>`arn:aws:iam::aws:policy/AmazonEKSVPCResourceController`<br/> + 下記のタグ用追加カスタムポリシー |
| ノード用IAMロール | `arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy`<br/>`arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly` | `arn:aws:iam::aws:policy/AmazonEKSWorkerNodeMinimalPolicy`<br/>`arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly` |

EKS Auto ModeのNodeClassで追加タグを設定する場合、Auto ModeのEKSクラスター用IAM Roleに以下のカスタムポリシーを追加する必要があります。このポリシーは、KarpenterであればKarpenterコントローラー用のポリシーに[相当](https://karpenter.sh/docs/reference/cloudformation/#karpenternoderole)しています。

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Compute",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateFleet",
                "ec2:RunInstances",
                "ec2:CreateLaunchTemplate"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/eks:eks-cluster-name": "{{ClusterName}}"
                },
                "StringLike": {
                    "aws:RequestTag/eks:kubernetes-node-class-name": "*",
                    "aws:RequestTag/eks:kubernetes-node-pool-name": "*"
                }
            }
        },
        {
            "Sid": "Storage",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateVolume",
                "ec2:CreateSnapshot"
            ],
            "Resource": [
                "arn:aws:ec2:*:*:volume/*",
                "arn:aws:ec2:*:*:snapshot/*"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/eks:eks-cluster-name": "{{ClusterName}}"
                }
            }
        },
        {
            "Sid": "Networking",
            "Effect": "Allow",
            "Action": "ec2:CreateNetworkInterface",
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/eks:eks-cluster-name": "{{ClusterName}}"
                },
                "StringLike": {
                    "aws:RequestTag/eks:kubernetes-cni-node-name": "*"
                }
            }
        },
        {
            "Sid": "LoadBalancer",
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:CreateLoadBalancer",
                "elasticloadbalancing:CreateTargetGroup",
                "elasticloadbalancing:CreateListener",
                "elasticloadbalancing:CreateRule",
                "ec2:CreateSecurityGroup"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/eks:eks-cluster-name": "{{ClusterName}}"
                }
            }
        },
        {
            "Sid": "ShieldProtection",
            "Effect": "Allow",
            "Action": [
                "shield:CreateProtection"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/eks:eks-cluster-name": "{{ClusterName}}"
                }
            }
        },
        {
            "Sid": "ShieldTagResource",
            "Effect": "Allow",
            "Action": [
                "shield:TagResource"
            ],
            "Resource": "arn:aws:shield::*:protection/*",
            "Condition": {
                "StringEquals": {
                    "aws:RequestTag/eks:eks-cluster-name": "{{ClusterName}}"
                }
            }
        }
    ]
}
```


## ネットワーク周りで制約がある

EKS Auto Modeは以下をサポートしています。

* EKSネットワークポリシー
* KubernetesポッドのHostPortおよびHostNetworkオプション
* パブリックサブネットまたはプライベートサブネットのポッド

一方で、以下をサポートしていないためネットワーク周りの制約があります。

* ポッドあたりのセキュリティグループ (SGPP)
* カスタムネットワーキング。ポッドとノードのIPアドレスは同じCIDRブロックからのものである必要がある
* ウォームIP、ウォームプレフィックス、ウォームENI設定
* 最小IPターゲット設定
* プレフィックス委任の有効化または無効化
* オープンソースAWS CNIでサポートされているその他の設定
* conntrackタイマーのカスタマイズなどのネットワークポリシー設定 (デフォルトは300秒)
* クラウドWatchへのネットワークイベントログのエクスポート

**運用中にサブネットのIPが足りなくなった時の追加VPC CIDR対応**

EKS Auto ModeのポッドとノードのIPアドレスは「同じCIDRブロックからのもの」である必要があります。[カスタムネットワーキング](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/cni-custom-network.html)をサポートしていないので、「運用中にVPCへ追加CIDRを設定、サブネットを切ってPodを起動させる」ときに注意が必要です。

NodeClass`default`はEKSクラスターと同じサブネットをIdで参照しており追加サブネットに対応できません。これに対応するには、独自C2NodeClassを作成して`タグでサブネットを探索`させましょう。

**EKS Nodeの最大Pod数が110に制限されている**

Auto ModeはEKSの最大Pod数が[「ハードリミットの110」か「ノードの最大ポッド数」の低い方に制限](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/choosing-instance-type.html)されています。

従来、VPC CNIを用いたEKSにおいてNodeごとに起動できるPodの数はENIに依存していました。この時、インスタンスファミリーごとの起動可能なPod数一覧は[GitHubで公開](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/misc/eni-max-pods.txt)されています。このため、インスタンスタイプによっては110を超えて起動できたのですが、Auto Modeでは110というハードリミットが設けられていることに注意が必要です。

## 組み込みでALBを作成できる

EKS Auto Modeは組み込みでALBを作成して構成できるため、AWS LoadBalancer Controller(ALBC)を別途インストールする必要がありません。ALBの作成はIngressリソースを使って行い、NLBの作成はServiceリソースを使って行います。

ALBを作成する手順は[ドキュメント](https://docs.aws.amazon.com/eks/latest/userguide/auto-configure-alb.html)に沿って、IngressClassParams、IngressClass、Ingressの3リソースを作成します。次の例は`Nginx`をALBで公開するリソースを作成します。

```yaml
# IngressClassParamsとIngressClassは非Namespaceリソース
apiVersion: eks.amazonaws.com/v1
kind: IngressClassParams
metadata:
  name: alb
spec:
  scheme: internet-facing
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: alb
  annotations:
    # Use this annotation to set an IngressClass as Default
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  # Configures the IngressClass to use EKS Auto Mode
  controller: eks.amazonaws.com/alb
  parameters:
    apiGroup: eks.amazonaws.com
    kind: IngressClassParams
    name: alb # IngressClassParams name
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: default
spec:
  ingressClassName: alb # match IngressClass
  rules:
    - http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: nginx-svc
                port:
                  number: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
spec:
  selector:
    app: nginx
  ports:
  - port: 8080
    targetPort: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: public.ecr.aws/nginx/nginx:1.27
        resources:
          limits:
            memory: "128Mi"
            cpu: "256m"
        ports:
        - name: http
          containerPort: 80
```

ALBCではingressのannotationsに様々なパラメーターを設定できましたが、EKS Auto ModeではIngressClassParamsやIngressClassで設定します。ALBCとはフィールドキーが変わったものもあるので注意が必要しましょう。

![image](https://github.com/user-attachments/assets/c342846f-2aa5-4849-bf98-9fc4e2fffdbc)

![image](https://github.com/user-attachments/assets/0febb967-7dab-4d85-987b-a912086ee570)

# 気になる挙動を試す

触っててどうなるのか気になることを色々触ってみます。

## EKS作成直後のPod状態

EKSアドオンを何も導入していない状態だとPodが存在しません。ユーザー側のアプリケーションや追加EKSアドオンがなければ常時必要なノードがないのはコスト的に嬉しいことです。納得いってなかったですよね?

```sh
$ kubectl get po -A
No resources found
$ kubectl get node
No resources found
$ kubectl get nodeclass
No resources found
$ kubectl get nodepool
```

## 追加EKSアドオンをインストールする

追加EKSアドオンをインストールすると、組み込みNodeGroupの`system`が利用されます。例えばMetrics Serverを追加すると、NodePool`system`でPodが起動します。

```sh
$ kubectl get po -A -o wide
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE
kube-system   metrics-server-746f5dc94-rdlzd   1/1     Running   0          64s
kube-system   metrics-server-746f5dc94-vl4wr   1/1     Running   0          3m15s

$ kubectl get node
NAME                  STATUS   ROLES    AGE   VERSION
i-095d98cf6f997a6a6   Ready    <none>   52s   v1.32.0-eks-2e66e76
i-09eabe62ba3931e7d   Ready    <none>   50s   v1.32.0-eks-2e66e76

$ kubectl get nodeclaim
NAME                TYPE        CAPACITY    ZONE              NODE                  READY     AGE
system-bcsln        c6g.large   on-demand   ap-northeast-1a   i-095d98cf6f997a6a6   True      77s
system-z7ppv        c6g.large   on-demand   ap-northeast-1c   i-09eabe62ba3931e7d   True      77s
```

## カスタムNodePoolはweightを着けたほうがいい。

組み込みNodePool`system`や`general-purpose`には優先度`spec.weight`が設定されていません。このため、カスタムNodePoolに優先度を指定しないと組み込みNodePoolとカスタムNodePoolでPodの起動ノードが混在します。

これを避けるためには、カスタムNodePoolに「適切な優先度をつける」「taintsをつける」のどちらかを検討するといいでしょう。そのNodePoolに他のPodを起動させたくないなら「taints」がいいでしょうし、そのPodをそのNodePoolで起動できればいいなら「NodePoolの優先度」か「nodeSelectorでNodePool名を指定」などでもいいでしょう。

```yaml
# 優先度をつける例
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: custom-pool
spec:
  # ... 省略
  weight: 10
```

```yaml
# taintsをつける例
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: custom-pool
spec:
  template:
    spec:
      # ... 省略
      taints:
      - effect: NoSchedule
        key: AppOnly
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      # taintsに合わせる
      tolerations:
      - key: AppOnly
        operator: Exists
      containers:
      - name: nginx
        image: public.ecr.aws/nginx/nginx:1.27
        resources:
          limits:
            memory: "128Mi"
            cpu: "256m"
        ports:
        - name: http
          containerPort: 80
```

## Auto Modeが管理するEC2はコンソールから消せない

従来のEKSでは、NodeGroupやKarpenterで起動したEC2はAWSコンソールから消せました。しかしEKS Auto Modeで起動したNode(EC2)をAWSコンソールからTerminate Instanceしようとしても実行失敗します。

![image](https://github.com/user-attachments/assets/14108830-2a86-4a87-9f8b-5aacac986a66)

```
Failed to terminate (delete) an instance: You are not authorized to perform this operation. User: arn:aws:sts::1234567801234:assumed-role/foo-role/bar is not authorized to perform: ec2:TerminateInstances on resource: arn:aws:ec2:ap-northeast-1:1234567801234:instance/i-04df53f6f7a5bc3d0 with an explicit deny in a resource-based policy. Encoded authorization failure message: ppuIku-省略...
```

kubectlからnodeを消すとEC2も連動して消えます。

```sh
$ kubectl delete node i-04df53f6f7a5bc3d0
node "i-04df53f6f7a5bc3d0" deleted
```

![image](https://github.com/user-attachments/assets/f1125c36-b12e-45d7-a6ca-eb045ed6e176)

## Auto Modeが管理するALBはコンソールから消せる

EKS Auto Modeが管理するALBはAWSコンソールから消せます。ここはEC2と挙動が違いますね。

> Successfully deleted load balancer: arn:aws:elasticloadbalancing:ap-northeast-1:1234567801234:loadbalancer/app/k8s-default-testingr-1234567801/abcdefghijklmnop.

従来通り、kubectlからingressを消すとALBも連動して消えます。

```sh
$ kubectl delete ingress test-ingress
ingress.networking.k8s.io "test-ingress" deleted
```

## Auto ModeのEKSクラスター削除時にEC2とALBはどうなるのか

通常のEKSでは、Karpenterで管理しているNodeを残したままEKSクラスターを消すとEC2が残ります。ALBCで管理しているingressリソースを残したままEKSクラスターを削除するとALBが残りました。一方で、EKS Auto Modeで作ったEC2はEKSクラスター削除に連動してEC2も消えます。同様にALBもEKSクラスターが削除されるときに連動して消されます。

表にまとめると次の通りです。

| EKSの動作モード | EKSクラスター削除時のEC2挙動 | EKSクラスター削除時のALB挙動 |
| --- | --- | --- |
| 通常のEKS | EC2が残る | ALBが残る |
| EKS Auto Mode | EC2が消える | ALBが消える |

## KarpenterとEKS Auto Modeで起動したEC2の違い

次の違いがあります。

| 項目 | Karpenter | EKS Auto Mode |
| --- | --- | --- |
| Nameタグ | Nameタグがつく | Nameタグがつかない |
| EC2のManaged状態 | false | true |
| EC2のボリューム | `/dev/xvda`のサイズ変更 | `/dev/xvdb`のサイズ変更 |

先に紹介した通り、EKS Auto Modeで起動したEC2をAWSコンソールから見ると`Managed: true`になっています。Karpenterで起動したEC2は`Managed: false`です。

![image](https://github.com/user-attachments/assets/dc8e8a08-1dca-4c01-a341-4f73694a062a)

EKS Auto Modeで起動したEC2にNameタグがつきません。Karpenterは`Name`タグにインスタンスの`IP name` (この例なら`ip-10-100-21-142.ap-northeast-1.compute.internal`)を設定します。(1段目の図)しかしEKS Auto Modeは`Name`タグ自体が設定されません。(2段目の図)

![image](https://github.com/user-attachments/assets/26fe9f87-06d3-49bc-8dc8-4e476baf5bfb)

![image](https://github.com/user-attachments/assets/83f20c8c-5025-4da2-aabe-17a22ffd57c3)

設定次第ではありますが、EC2のボリュームも違います。KarpenterではEC2NodeClassでボリュームサイズを指定すると`/dev/xvda`のサイズが変わりました。しかしEKS Auto Modeでは、EC2へストレージ`/dev/xvda`と`/dev/xvdb`の2つアタッチされ、NodeClassでボリュームサイズを指定すると`/dev/xvdb`のサイズが変わります。例えばカスタムNodeClassで20GiBを指定すると、`/dev/xvdb`EBSのサイズが20GiBになっていますね。

![image](https://github.com/user-attachments/assets/8f5f6b8e-9bc9-46b0-aedf-9c58a916fb3e)

```yaml
apiVersion: eks.amazonaws.com/v1
kind: NodeClass
metadata:
  name: custom-class
spec:
  # 省略
  ephemeralStorage:
    size: "20Gi" # <-- ここでサイズ指定
```

![image](https://github.com/user-attachments/assets/7f300de6-e32f-4f6e-b473-26fa91db131e)

## EKS AccessEntryの注意

Built-in NodeGroupを作っていると、IAM access entriesにNodeGroupで指定したNode用のIAM RoleArn(通常はAmazonEKSAutoNodeRole)が追加されます。

![image](https://github.com/user-attachments/assets/62a41cb8-855c-4aae-918c-093fc670f9d8)

しかしBuilt-in NodeGroupを作らない場合、カスタムNodeClassを展開するまではIAM access entriesにAmazonEKSAutoNodeRoleが追加されません。
一見自分で追加しないといけないのか!? となりますが、カスタムNodePoolを展開すると10-15分程度で自動追加されます。
早まってIaCで追加しないようにしましょう。

## NodeClassの注意

**spec.roleにはIAM Role名を指定する**

spec.roleのIAM Role名は64文字未満である必要があります。[ドキュメント](https://docs.aws.amazon.com/eks/latest/userguide/create-node-class.html)がIAM Role ARNになっているので64文字に収まらずびっくりしますが、実際はIAM Role Name指定です。

```yaml
# ×: ドキュメントはIAM Role ARN表記になっているがダメ
spec:
  role: arn:aws:iam::123456789012:role/MyNodeRole
  # 〇: 実際はIAM Role NAmeでOK。OSS Karpenterと同じ
  role: MyNodeRole
```

```sh
$ kubectl apply -f ./k8s/test/automode_nodepool.yaml
The NodeClass "custom-class" is invalid:
* spec.role: Too long: may not be more than 64 bytes
* <nil>: Invalid value: "null": some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
```

```yaml
# 〇: 実際はIAM Role NAmeでOK。OSS Karpenterと同じ
spec:
  role: MyNodeRole
```

**Auto Mode管理のEC2に追加タグを設定するにはEKS Cluster Roleに追加権限が必要**

Auto Modeで起動するノードに追加タグを設定して見ましょう。NodeClaimが展開出来ず、nodeclaimをdescribeすると`rror getting launch template configs: User is not authorized to perform this operation because no identity-based policy allows it`と表示されている場合、EKSクラスターのIAM Roleに権限が足りません。この場合、[追加権限](https://docs.aws.amazon.com/eks/latest/userguide/auto-learn-iam.html#tag-prop)をIAM Roleに設定すれば、無事にNodeClaimが展開されて、EC2起動・Podスケジュールされます。

```yaml
apiVersion: eks.amazonaws.com/v1
kind: NodeClass
metadata:
  name: custom-class
spec:
  # 省略
  tags:
    environment: custom
```

```sh
$ kubectl describe nodeclaim xxxxx

Status:
  Conditions:
    Last Transition Time:  2025-03-12T09:48:38Z
    Message:               object is awaiting reconciliation
    Observed Generation:   1
    Reason:                AwaitingReconciliation
    Status:                Unknown
    Type:                  Initialized
    Last Transition Time:  2025-03-12T09:48:38Z
    Message:               Error getting launch template configs: User is not authorized to perform this operation because no identity-based policy allows it
    Observed Generation:   1
    Reason:                Unauthorized
    Status:                Unknown
    Type:                  Launched
    Last Transition Time:  2025-03-12T09:48:38Z
    Message:               Node not registered with cluster
    Observed Generation:   1
    Reason:                NodeNotFound
    Status:                Unknown
    Type:                  Registered
    Last Transition Time:  2025-03-12T09:48:38Z
    Message:               Initialized=Unknown, Launched=Unknown, Registered=Unknown
    Observed Generation:   1
    Reason:                ReconcilingDependents
    Status:                Unknown
    Type:                  Ready
Events:                    <none>
```


## NodePoolの注意

Karpenterでは、expireAfterにNeverを設定して寿命で入れ替わらないように出来ました。EKS Auto Modeで`spec.template.spec.expireAfter`を`Never`に設定するとエラーになります。これは先に説明した最大21日という制限に引っかかるためです。

```sh
$ kubectl apply -f ./k8s/test/automode_nodepool.yaml
Error from server (Invalid): error when creating "./k8s/test/automode_nodepool.yaml": NodePool.karpenter.sh "custom-pool" is invalid: [spec.template.spec: Invalid value: "object": type conversion error from 'string' to 'google.protobuf.Duration' evaluating rule: the sum of expireAfter and terminationGracePeriod may not exceed 21 days, spec.template.spec: Invalid value: "object": expireAfter may not be set to Never]
```

# 参考

AWSドキュメント

* [Amazon EKS Auto Mode の発表 | AWS](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/amazon-eks-auto-mode/)
* [Getting started with Amazon EKS Auto Mode | Containers](https://aws.amazon.com/jp/blogs/containers/getting-started-with-amazon-eks-auto-mode/)
* [Create a Node Pool for EKS Auto Mode - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/create-node-pool.html)
* [Create an EKS Auto Mode Cluster with the AWS Management Console - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/automode-get-started-console.html)
* [Learn about Amazon EKS Auto Mode Managed instances - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/automode-learn-instances.html)
* [Learn about identity and access in EKS Auto Mode - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/auto-learn-iam.html)
* [Learn about VPC Networking and Load Balancing in EKS Auto Mode - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/auto-networking.html)
* [Deploy a sample inflate workload to an Amazon EKS Auto Mode cluster - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/automode-workload.html)
* [Choose an optimal Amazon EC2 node instance type - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/choosing-instance-type.html)
* [Community add-ons - Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/community-addons.html)

AWSスライド

* [EKS Auto Mode | Spearker Deck](https://speakerdeck.com/kashinoki38/eks-auto-mode)
* [AWS Blackbelt Online Seminarre:Invent 2024 アップデート速報](https://pages.awscloud.com/rs/112-TZM-766/images/AWS-Black-Belt_2024_AWS-reInvent_1206_v1.pdf)

その他

* [Karpenter NodeClaim has error “Error getting launch template configs” in Amazon EKS Auto Mode | by Hiroaki Kaji | Medium](https://medium.com/@kazioyazi/nodeclaim-has-error-error-getting-launch-template-configs-in-eks-auto-mode-46edca067fba)
* [Terraform で Auto Mode の EKS クラスターを作成して、Pod Identity で AWS 権限を与えたアプリケーションをデプロイしてみた | DevelopersIO](https://dev.classmethod.jp/articles/create-eks-auto-mode-cluster-by-terraform/)
* [EKS Auto Mode で独自の NodePool を作成して利用してみた | DevelopersIO](https://dev.classmethod.jp/articles/eks-auto-mode-custom-node-pool/)
* [[メモ]Amazon EKS Auto Modeの所感](https://zenn.dev/fy0323/articles/4d64ebd5195cdd)
* [NodeClasses | Karpenter](https://karpenter.sh/docs/concepts/nodeclasses/)
* [NodePools | Karpenter](https://karpenter.sh/docs/concepts/nodepools/)
* [[EKS] [request]: Support custom TargetGroupBinding resources in auto-mode - Issue #2508 | aws/containers-roadmap](https://github.com/aws/containers-roadmap/issues/2508)


[^1]: KarpenterのEC2NodeClassに相当しますが、EKS Auto Modeでは設定できるフィールドに制限があります
[^2]: ECSもFargateだとお高くなりがちなのと似ていますが、ECS EC2では追加コストないので非対称な価格設定です。
[^3]: 自分で[LCUを設定できる](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/capacity-unit-reservation.html)ようになってよかったよかった。
[^4]: Managed NodeGroupを0台にして、Karpenterで起動したEC2を強制的にTerminateすることで簡単に夜間停止を用意できたのが不可能になった
[^5]: EKS with Fargateはコスト面で最も悪いですが同軸には適さないので除外
[^6]: そもそもHelmコントローラーの更新がつらいので0にしたい
[^7]: Karpenterコントローラーを自分で使う場合、先にEC2NodeClass、NodePoolリソースを展開しておくのでPodがいなくても見えるはず。ないということは、Pod作成をトリガーにNodeClassやNodePoolを作っていると推察される。
[^8]: 2台起動するのは、Metrics ServerのpodAntiAffinityで`affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution`にて`"topologyKey": "kubernetes.io/hostname"`が設定されているため同一Nodeでスケジュールされないため
